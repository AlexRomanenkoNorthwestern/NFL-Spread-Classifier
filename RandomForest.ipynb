{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2149688,"status":"error","timestamp":1701055821642,"user":{"displayName":"Alexander Romanenko","userId":"05913195296226834875"},"user_tz":360},"id":"aeAodS3I0ssw","outputId":"e3f1b8b2-8a9e-40b5-a134-134d3246601f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Progress = 0.2 %\n","Progress = 0.7 %\n","Progress = 1.2 %\n","Progress = 1.7 %\n","Progress = 2.2 %\n","Progress = 2.7 %\n","Progress = 3.2 %\n","Progress = 3.7 %\n","Progress = 4.2 %\n","Progress = 4.7 %\n","Progress = 5.2 %\n","Progress = 5.7 %\n","Progress = 6.2 %\n","Progress = 6.7 %\n","Progress = 7.2 %\n","Progress = 7.7 %\n","Progress = 8.2 %\n","Progress = 8.7 %\n","Progress = 9.2 %\n","Progress = 9.7 %\n","Progress = 10.2 %\n","Progress = 10.7 %\n","Progress = 11.2 %\n","Progress = 11.7 %\n","Progress = 12.2 %\n","Progress = 12.7 %\n","Progress = 13.2 %\n","Progress = 13.7 %\n","Progress = 14.2 %\n","Progress = 14.7 %\n","Progress = 15.2 %\n","Progress = 15.7 %\n","Progress = 16.2 %\n","Progress = 16.7 %\n","Progress = 17.2 %\n","Progress = 17.7 %\n","Progress = 18.2 %\n","Progress = 18.7 %\n","Progress = 19.2 %\n","Progress = 19.7 %\n","Progress = 20.2 %\n","Progress = 20.7 %\n","Progress = 21.2 %\n","Progress = 21.7 %\n","Progress = 22.2 %\n","Progress = 22.7 %\n","Progress = 23.2 %\n","Progress = 23.7 %\n","Progress = 24.2 %\n","Progress = 24.7 %\n","Progress = 25.2 %\n","Progress = 25.7 %\n","Progress = 26.2 %\n","Progress = 26.7 %\n","Progress = 27.2 %\n","Progress = 27.7 %\n","Progress = 28.2 %\n","Progress = 28.7 %\n","Progress = 29.2 %\n","Progress = 29.7 %\n","Progress = 30.2 %\n","Progress = 30.7 %\n","Progress = 31.2 %\n","Progress = 31.7 %\n","Progress = 32.2 %\n","Progress = 32.7 %\n","Progress = 33.2 %\n","Progress = 33.7 %\n","Progress = 34.2 %\n","Progress = 34.7 %\n","Progress = 35.2 %\n","Progress = 35.7 %\n","Progress = 36.2 %\n","Progress = 36.7 %\n","Progress = 37.2 %\n","Progress = 37.7 %\n","Progress = 38.2 %\n","Progress = 38.7 %\n","Progress = 39.2 %\n","Progress = 39.7 %\n","Progress = 40.2 %\n","Progress = 40.7 %\n","Progress = 41.2 %\n","Progress = 41.7 %\n","Progress = 42.2 %\n","Progress = 42.7 %\n","Progress = 43.2 %\n","Progress = 43.7 %\n","Progress = 44.2 %\n","Progress = 44.7 %\n","Progress = 45.2 %\n","Progress = 45.7 %\n","Progress = 46.2 %\n","Progress = 46.7 %\n","Progress = 47.2 %\n","Progress = 47.7 %\n","Progress = 48.2 %\n","Progress = 48.7 %\n","Progress = 49.2 %\n","Progress = 49.7 %\n","Progress = 50.2 %\n","Progress = 50.7 %\n","Progress = 51.2 %\n","Progress = 51.7 %\n","Progress = 52.2 %\n","Progress = 52.7 %\n","Progress = 53.2 %\n","Progress = 53.7 %\n","Progress = 54.2 %\n","Progress = 54.7 %\n","Progress = 55.2 %\n","Progress = 55.7 %\n","Progress = 56.2 %\n","Progress = 56.7 %\n","Progress = 57.2 %\n","Progress = 57.7 %\n","Progress = 58.2 %\n","Progress = 58.7 %\n","Progress = 59.2 %\n","Progress = 59.7 %\n","Progress = 60.2 %\n","Progress = 60.7 %\n","Progress = 61.2 %\n","Progress = 61.7 %\n","Progress = 62.2 %\n","Progress = 62.7 %\n","Progress = 63.2 %\n","Progress = 63.7 %\n","Progress = 64.2 %\n","Progress = 64.7 %\n","Progress = 65.2 %\n","Progress = 65.7 %\n","Progress = 66.2 %\n","Progress = 66.7 %\n","Progress = 67.2 %\n","Progress = 67.7 %\n","Progress = 68.2 %\n","Progress = 68.7 %\n","Progress = 69.2 %\n","Progress = 69.7 %\n","Progress = 70.2 %\n","Progress = 70.7 %\n","Progress = 71.2 %\n","Progress = 71.7 %\n","Progress = 72.2 %\n","Progress = 72.7 %\n","Progress = 73.2 %\n","Progress = 73.7 %\n","Progress = 74.2 %\n","Progress = 74.7 %\n","Progress = 75.2 %\n","Progress = 75.7 %\n","Progress = 76.2 %\n","Progress = 76.7 %\n","Progress = 77.2 %\n","Progress = 77.7 %\n","Progress = 78.2 %\n","Progress = 78.7 %\n","Progress = 79.2 %\n","Progress = 79.7 %\n","Progress = 80.2 %\n","Progress = 80.7 %\n","Progress = 81.2 %\n","Progress = 81.7 %\n","Progress = 82.2 %\n","Progress = 82.7 %\n","Progress = 83.2 %\n","Progress = 83.7 %\n","Progress = 84.2 %\n","Progress = 84.7 %\n","Progress = 85.2 %\n","Progress = 85.7 %\n","Progress = 86.2 %\n","Progress = 86.7 %\n","Progress = 87.2 %\n","Progress = 87.7 %\n","Progress = 88.2 %\n","Progress = 88.7 %\n","Progress = 89.2 %\n","Progress = 89.7 %\n","Progress = 90.2 %\n","Progress = 90.7 %\n","Progress = 91.2 %\n","Progress = 91.7 %\n","Progress = 92.2 %\n","Progress = 92.7 %\n","Progress = 93.2 %\n","Progress = 93.7 %\n","Progress = 94.2 %\n","Progress = 94.7 %\n","Progress = 95.2 %\n","Progress = 95.7 %\n","Progress = 96.2 %\n","Progress = 96.7 %\n","Progress = 97.2 %\n","Progress = 97.7 %\n","Progress = 98.2 %\n","Progress = 98.7 %\n","Progress = 99.2 %\n","Progress = 99.7 %\n","Avg Train Accuracy: 0.620515759312321\n","Avg Valid Accuracy: 0.49758713136729227\n","Avg Test Accuracy: 0.509624234777996\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-99-3e8a36e78373>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0mpooled_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m   \u001b[0mpredicted_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0mpredicted_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"]}],"source":["from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np\n","import copy\n","import random\n","\n","# Read the CSV file into a DataFrame\n","data = pd.read_csv('sample_data/Final_Data_NFL.csv')\n","\n","# Creating a new DataFrame with all columns except 'column2' and 'column4'\n","columns_to_exclude = ['Day', 'Date', 'Time', 'Favorite', 'FavoriteTeamYear', 'UnderdogTeamYear',\n","                      'Score', 'Underdog', 'OverUnder', 'Season_Year', 'Favorite_Covered']\n","X = data.drop(columns=columns_to_exclude)\n","Y = data[\"Favorite_Covered\"]\n","\n","# Convert time of posession columns to numbers\n","for col_name in ['Favoriteaverage-time-of-possession-net-of-ot', 'Favoriteopponent-average-time-of-possession-net-of-ot', 'Underdogaverage-time-of-possession-net-of-ot', 'Underdogopponent-average-time-of-possession-net-of-ot']:\n","  col_data = X[col_name]\n","  new_col_data = []\n","  for row in col_data:\n","    split_data = row.split(':')\n","    col_data = 60*int(split_data[0]) + int(split_data[1])\n","    new_col_data.append(col_data)\n","  X[col_name] = new_col_data\n","\n","# Convert % to float\n","for col_name in [\n","    \"Favoritered-zone-scoring-pct\", \"Favoritethird-down-conversion-pct\", \"Favoritefield-goal-conversion-pct\",\n","    \"Favoriteopponent-red-zone-scoring-pct\", \"Favoriteopponent-third-down-conversion-pct\", \"Favoriteopponent-completion-pct\",\n","    \"Favoriteopponent-field-goal-conversion-pct\", \"Underdogred-zone-scoring-pct\", \"Underdogthird-down-conversion-pct\",\n","    \"Underdogfield-goal-conversion-pct\", \"Underdogopponent-red-zone-scoring-pct\", \"Underdogopponent-third-down-conversion-pct\",\n","    \"Underdogopponent-completion-pct\", \"Underdogopponent-field-goal-conversion-pct\"\n","]:\n","  col_data = X[col_name]\n","  new_col_data = []\n","  for row in col_data:\n","    col_data = row[0:-1]\n","    new_col_data.append(col_data)\n","  X[col_name] = new_col_data\n","\n","# If there is not a reported spread, get rid of the row of data\n","indices = []\n","for i, row in enumerate(X['Spread']):\n","  if str(row) == 'nan':\n","    indices.append(i)\n","X = X.drop(indices)\n","Y = Y.drop(indices)\n","\n","\n","# If there is a push and both sides of bet get their money back -set the label to P\n","new_Y = []\n","for row in Y:\n","  if row == 'T' or row == 'F':\n","    val = row\n","  else:\n","    val = 'T'\n","  new_Y.append(val)\n","Y = pd.Series(new_Y)\n","\n","# Splitting the data into training and testing sets(70%, 15%, 15%) - 5 times\n","# We will choose the most classified label provided by the 5 random forests when making predictions\n","X_temp1, X_test, y_temp1, y_test= train_test_split(X, Y, test_size=0.15, random_state=1)\n","X_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=1)\n","X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=2)\n","X_train3, X_valid3, y_train3, y_valid3 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=3)\n","X_train4, X_valid4, y_train4, y_valid4 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=4)\n","X_train5, X_valid5, y_train5, y_valid5 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=5)\n","\n","best_valid_accuracy = 0\n","best_classifier1 = None\n","best_classifier2 = None\n","best_classifier3 = None\n","best_classifier4 = None\n","best_classifier5 = None\n","best_criterion = None\n","best_min_samples_split = None\n","best_max_features = None\n","# Creating a Random Forest classifier - based on 100 decision trees\n","criterion_iter = -1\n","for criterion in [\"gini\", \"entropy\"]:\n","  criterion_iter += 1\n","  for min_samples_split in range(2, 500, 5):\n","    print(\"Progress =\", round(min_samples_split/1000 * 100 + (criterion_iter*50),2), \"%\")\n","    for max_features in [10, 20, 30, 40, 50, \"sqrt\"]:\n","      rf_classifier1 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=1)\n","      rf_classifier2 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=2)\n","      rf_classifier3 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=3)\n","      rf_classifier4 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=4)\n","      rf_classifier5 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=5)\n","\n","      # Training the classifier\n","      rf_classifier1.fit(X_train1, y_train1)\n","      rf_classifier2.fit(X_train2, y_train2)\n","      rf_classifier3.fit(X_train3, y_train3)\n","      rf_classifier4.fit(X_train4, y_train4)\n","      rf_classifier5.fit(X_train5, y_train5)\n","\n","      # Predicting on the valid set\n","      predictions_valid1 = rf_classifier1.predict(X_valid1)\n","      predictions_valid2 = rf_classifier2.predict(X_valid2)\n","      predictions_valid3 = rf_classifier3.predict(X_valid3)\n","      predictions_valid4 = rf_classifier4.predict(X_valid4)\n","      predictions_valid5 = rf_classifier5.predict(X_valid5)\n","\n","      # Calculating accuracy - valid\n","      valid_accuracy1 = accuracy_score(y_valid1, predictions_valid1)\n","      valid_accuracy2 = accuracy_score(y_valid2, predictions_valid2)\n","      valid_accuracy3 = accuracy_score(y_valid3, predictions_valid3)\n","      valid_accuracy4 = accuracy_score(y_valid4, predictions_valid4)\n","      valid_accuracy5 = accuracy_score(y_valid5, predictions_valid5)\n","      valid_accuracy = (valid_accuracy1 + valid_accuracy2 + valid_accuracy3 + valid_accuracy4 + valid_accuracy5)/5\n","\n","      if valid_accuracy > best_valid_accuracy:\n","        best_valid_accuracy = valid_accuracy\n","        best_classifier1 = copy.deepcopy(rf_classifier1)\n","        best_classifier2 = copy.deepcopy(rf_classifier2)\n","        best_classifier3 = copy.deepcopy(rf_classifier3)\n","        best_classifier4 = copy.deepcopy(rf_classifier4)\n","        best_classifier5 = copy.deepcopy(rf_classifier5)\n","        best_criterion = criterion\n","        best_min_samples_split = min_samples_split\n","        best_max_features = max_features\n","\n","# Predicting on the train set\n","predictions_train1 = rf_classifier1.predict(X_train1)\n","predictions_train2 = rf_classifier2.predict(X_train2)\n","predictions_train3 = rf_classifier3.predict(X_train3)\n","predictions_train4 = rf_classifier4.predict(X_train4)\n","predictions_train5 = rf_classifier5.predict(X_train5)\n","\n","train_accuracy1 = accuracy_score(y_train1, predictions_train1)\n","train_accuracy2 = accuracy_score(y_train2, predictions_train2)\n","train_accuracy3 = accuracy_score(y_train3, predictions_train3)\n","train_accuracy4 = accuracy_score(y_train4, predictions_train4)\n","train_accuracy5 = accuracy_score(y_train5, predictions_train5)\n","train_accuracy = (train_accuracy1 + train_accuracy2 + train_accuracy3 + train_accuracy4 + train_accuracy5)/5\n","print(f\"Avg Train Accuracy: {train_accuracy}\")\n","\n","# Predicting on the valid set\n","predictions_valid1 = rf_classifier1.predict(X_valid1)\n","predictions_valid2 = rf_classifier2.predict(X_valid2)\n","predictions_valid3 = rf_classifier3.predict(X_valid3)\n","predictions_valid4 = rf_classifier4.predict(X_valid4)\n","predictions_valid5 = rf_classifier5.predict(X_valid5)\n","\n","valid_accuracy1 = accuracy_score(y_valid1, predictions_valid1)\n","valid_accuracy2 = accuracy_score(y_valid2, predictions_valid2)\n","valid_accuracy3 = accuracy_score(y_valid3, predictions_valid3)\n","valid_accuracy4 = accuracy_score(y_valid4, predictions_valid4)\n","valid_accuracy5 = accuracy_score(y_valid5, predictions_valid5)\n","valid_accuracy = (valid_accuracy1 + valid_accuracy2 + valid_accuracy3 + valid_accuracy4 + valid_accuracy5)/5\n","print(f\"Avg Valid Accuracy: {valid_accuracy}\")\n","\n","# Predicting on the test set\n","predictions_test1 = rf_classifier1.predict(X_test)\n","predictions_test2 = rf_classifier2.predict(X_test)\n","predictions_test3 = rf_classifier3.predict(X_test)\n","predictions_test4 = rf_classifier4.predict(X_test)\n","predictions_test5 = rf_classifier5.predict(X_test)\n","\n","test_accuracy1 = accuracy_score(y_test, predictions_test1)\n","test_accuracy2 = accuracy_score(y_test, predictions_test2)\n","test_accuracy3 = accuracy_score(y_test, predictions_test3)\n","test_accuracy4 = accuracy_score(y_test, predictions_test4)\n","test_accuracy5 = accuracy_score(y_test, predictions_test5)\n","test_accuracy = (test_accuracy1 + valid_accuracy2 + valid_accuracy3 + valid_accuracy4 + valid_accuracy5)/5\n","print(f\"Avg Test Accuracy: {test_accuracy}\")"]},{"cell_type":"code","source":["pooled_predictions = []\n","for i in range(len(predictions_test1)):\n","  predicted_true = 0\n","  predicted_false = 0\n","  for prediction_list in [predictions_test1, predictions_test2, predictions_test3, predictions_test4, predictions_test5]:\n","    if prediction_list[i] == 'T':\n","       predicted_true += 1\n","    else:\n","      predicted_false += 1\n","  if predicted_true > predicted_false:\n","    pooled_predictions.append('T')\n","  else:\n","    pooled_predictions.append('F')\n","\n","pooled_test_accuracy = accuracy_score(y_test, pooled_predictions)\n","print(f\"Test Accuracy With Pooling: {pooled_test_accuracy}\")\n","\n","print(\"Criterion:\", best_criterion)\n","print(\"Min Samples To Split:\", best_min_samples_split)\n","print(\"Max Features:\", best_max_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_-W7ucF1IhW","executionInfo":{"status":"ok","timestamp":1701055862640,"user_tz":360,"elapsed":6,"user":{"displayName":"Alexander Romanenko","userId":"05913195296226834875"}},"outputId":"162f7dc1-adf8-4c24-9226-e463c234821d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy With Pooling: 0.5721925133689839\n","Criterion: gini\n","Min Samples To Split: 77\n","Max Features: 50\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1701044551213,"user":{"displayName":"Alexander Romanenko","userId":"05913195296226834875"},"user_tz":360},"id":"NK2ko8KxJUaZ","outputId":"73759d7e-c613-4412-a10b-2722fdc8f82b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.48128342245989303\n"]}],"source":["random_list = [random.choice([\"T\", \"F\"]) for _ in range(len(y_test))]\n","test_accuracy = accuracy_score(y_test, random_list)\n","print(f\"Accuracy: {test_accuracy}\")"]},{"cell_type":"code","source":["from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","import numpy as np\n","import copy\n","\n","# Read the CSV file into a DataFrame\n","data = pd.read_csv('sample_data/Final_Data_NFL.csv')\n","\n","# Creating a new DataFrame with all columns except 'column2' and 'column4'\n","columns_to_exclude = ['Day', 'Date', 'Time', 'Favorite', 'FavoriteTeamYear', 'UnderdogTeamYear',\n","                      'Score', 'Underdog', 'OverUnder', 'Season_Year', 'Favorite_Covered']\n","X = data.drop(columns=columns_to_exclude)\n","Y = data[\"Favorite_Covered\"]\n","\n","# Convert time of posession columns to numbers\n","for col_name in ['Favoriteaverage-time-of-possession-net-of-ot', 'Favoriteopponent-average-time-of-possession-net-of-ot', 'Underdogaverage-time-of-possession-net-of-ot', 'Underdogopponent-average-time-of-possession-net-of-ot']:\n","  col_data = X[col_name]\n","  new_col_data = []\n","  for row in col_data:\n","    split_data = row.split(':')\n","    col_data = 60*int(split_data[0]) + int(split_data[1])\n","    new_col_data.append(col_data)\n","  X[col_name] = new_col_data\n","\n","# Convert % to float\n","for col_name in [\n","    \"Favoritered-zone-scoring-pct\", \"Favoritethird-down-conversion-pct\", \"Favoritefield-goal-conversion-pct\",\n","    \"Favoriteopponent-red-zone-scoring-pct\", \"Favoriteopponent-third-down-conversion-pct\", \"Favoriteopponent-completion-pct\",\n","    \"Favoriteopponent-field-goal-conversion-pct\", \"Underdogred-zone-scoring-pct\", \"Underdogthird-down-conversion-pct\",\n","    \"Underdogfield-goal-conversion-pct\", \"Underdogopponent-red-zone-scoring-pct\", \"Underdogopponent-third-down-conversion-pct\",\n","    \"Underdogopponent-completion-pct\", \"Underdogopponent-field-goal-conversion-pct\"\n","]:\n","  col_data = X[col_name]\n","  new_col_data = []\n","  for row in col_data:\n","    col_data = row[0:-1]\n","    new_col_data.append(col_data)\n","  X[col_name] = new_col_data\n","\n","# If there is not a reported spread, get rid of the row of data\n","indices = []\n","for i, row in enumerate(X['Spread']):\n","  if str(row) == 'nan':\n","    indices.append(i)\n","X = X.drop(indices)\n","Y = Y.drop(indices)\n","\n","\n","# If there is a push and both sides of bet get their money back -set the label to P\n","new_Y = []\n","for row in Y:\n","  if row == 'T' or row == 'F':\n","    val = row\n","  else:\n","    val = 'T'\n","  new_Y.append(val)\n","Y = pd.Series(new_Y)\n","\n","# Splitting the data into training and testing sets(70%, 15%, 15%) - 5 times\n","# We will choose the most classified label provided by the 5 random forests when making predictions\n","X_temp1, X_test, y_temp1, y_test= train_test_split(X, Y, test_size=0.15, random_state=1)\n","X_train1, X_valid1, y_train1, y_valid1 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=1)\n","X_train2, X_valid2, y_train2, y_valid2 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=2)\n","X_train3, X_valid3, y_train3, y_valid3 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=3)\n","X_train4, X_valid4, y_train4, y_valid4 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=4)\n","X_train5, X_valid5, y_train5, y_valid5 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=5)\n","X_train6, X_valid6, y_train6, y_valid6 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=6)\n","X_train7, X_valid7, y_train7, y_valid7 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=7)\n","X_train8, X_valid8, y_train8, y_valid8 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=8)\n","X_train9, X_valid9, y_train9, y_valid9 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=9)\n","X_train10, X_valid10, y_train10, y_valid10 = train_test_split(X_temp1, y_temp1, test_size=0.176, random_state=10)\n","\n","best_valid_accuracy = 0\n","best_classifier1 = None\n","best_classifier2 = None\n","best_classifier3 = None\n","best_classifier4 = None\n","best_classifier5 = None\n","best_classifier6 = None\n","best_classifier7 = None\n","best_classifier8 = None\n","best_classifier9 = None\n","best_classifier10 = None\n","best_criterion = None\n","best_min_samples_split = None\n","best_max_features = None\n","# Creating a Random Forest classifier - based on 100 decision trees\n","criterion_iter = -1\n","for criterion in [\"gini\", \"entropy\"]:\n","  criterion_iter += 1\n","  for min_samples_split in range(2, 200, 5):\n","    print(\"Progress =\", round(min_samples_split/400 * 100 + (criterion_iter*50),2), \"%\")\n","    for max_features in [10, 20, 30, 40, 50, \"sqrt\"]:\n","      rf_classifier1 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=1)\n","      rf_classifier2 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=2)\n","      rf_classifier3 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=3)\n","      rf_classifier4 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=4)\n","      rf_classifier5 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=5)\n","      rf_classifier6 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=6)\n","      rf_classifier7 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=7)\n","      rf_classifier8 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=8)\n","      rf_classifier9 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=9)\n","      rf_classifier10 = RandomForestClassifier(n_estimators=20, criterion = criterion, min_samples_split = min_samples_split, max_features = max_features, random_state=10)\n","\n","      # Training the classifier\n","      rf_classifier1.fit(X_train1, y_train1)\n","      rf_classifier2.fit(X_train2, y_train2)\n","      rf_classifier3.fit(X_train3, y_train3)\n","      rf_classifier4.fit(X_train4, y_train4)\n","      rf_classifier5.fit(X_train5, y_train5)\n","      rf_classifier6.fit(X_train6, y_train6)\n","      rf_classifier7.fit(X_train7, y_train7)\n","      rf_classifier8.fit(X_train8, y_train8)\n","      rf_classifier9.fit(X_train9, y_train9)\n","      rf_classifier10.fit(X_train10, y_train10)\n","\n","      # Predicting on the valid set\n","      predictions_valid1 = rf_classifier1.predict(X_valid1)\n","      predictions_valid2 = rf_classifier2.predict(X_valid2)\n","      predictions_valid3 = rf_classifier3.predict(X_valid3)\n","      predictions_valid4 = rf_classifier4.predict(X_valid4)\n","      predictions_valid5 = rf_classifier5.predict(X_valid5)\n","      predictions_valid6 = rf_classifier5.predict(X_valid6)\n","      predictions_valid7 = rf_classifier5.predict(X_valid7)\n","      predictions_valid8 = rf_classifier5.predict(X_valid8)\n","      predictions_valid9 = rf_classifier5.predict(X_valid9)\n","      predictions_valid10 = rf_classifier5.predict(X_valid10)\n","\n","      # Calculating accuracy - valid\n","      valid_accuracy1 = accuracy_score(y_valid1, predictions_valid1)\n","      valid_accuracy2 = accuracy_score(y_valid2, predictions_valid2)\n","      valid_accuracy3 = accuracy_score(y_valid3, predictions_valid3)\n","      valid_accuracy4 = accuracy_score(y_valid4, predictions_valid4)\n","      valid_accuracy5 = accuracy_score(y_valid5, predictions_valid5)\n","      valid_accuracy6 = accuracy_score(y_valid6, predictions_valid6)\n","      valid_accuracy7 = accuracy_score(y_valid7, predictions_valid7)\n","      valid_accuracy8 = accuracy_score(y_valid8, predictions_valid8)\n","      valid_accuracy9 = accuracy_score(y_valid9, predictions_valid9)\n","      valid_accuracy10 = accuracy_score(y_valid10, predictions_valid10)\n","      valid_accuracy = (valid_accuracy1 + valid_accuracy2 + valid_accuracy3 + valid_accuracy4 + valid_accuracy5 + valid_accuracy6 + valid_accuracy7 + valid_accuracy8 + valid_accuracy9 + valid_accuracy10)/10\n","\n","      if valid_accuracy > best_valid_accuracy:\n","        best_valid_accuracy = valid_accuracy\n","        best_classifier1 = copy.deepcopy(rf_classifier1)\n","        best_classifier2 = copy.deepcopy(rf_classifier2)\n","        best_classifier3 = copy.deepcopy(rf_classifier3)\n","        best_classifier4 = copy.deepcopy(rf_classifier4)\n","        best_classifier5 = copy.deepcopy(rf_classifier5)\n","        best_classifier6 = copy.deepcopy(rf_classifier6)\n","        best_classifier7 = copy.deepcopy(rf_classifier7)\n","        best_classifier8 = copy.deepcopy(rf_classifier8)\n","        best_classifier9 = copy.deepcopy(rf_classifier9)\n","        best_classifier10 = copy.deepcopy(rf_classifier10)\n","\n","        best_criterion = criterion\n","        best_min_samples_split = min_samples_split\n","        best_max_features = max_features\n","\n","# Predicting on the train set\n","predictions_train1 = rf_classifier1.predict(X_train1)\n","predictions_train2 = rf_classifier2.predict(X_train2)\n","predictions_train3 = rf_classifier3.predict(X_train3)\n","predictions_train4 = rf_classifier4.predict(X_train4)\n","predictions_train5 = rf_classifier5.predict(X_train5)\n","predictions_train6 = rf_classifier6.predict(X_train6)\n","predictions_train7 = rf_classifier7.predict(X_train7)\n","predictions_train8 = rf_classifier8.predict(X_train8)\n","predictions_train9 = rf_classifier9.predict(X_train9)\n","predictions_train10 = rf_classifier10.predict(X_train10)\n","\n","train_accuracy1 = accuracy_score(y_train1, predictions_train1)\n","train_accuracy2 = accuracy_score(y_train2, predictions_train2)\n","train_accuracy3 = accuracy_score(y_train3, predictions_train3)\n","train_accuracy4 = accuracy_score(y_train4, predictions_train4)\n","train_accuracy5 = accuracy_score(y_train5, predictions_train5)\n","train_accuracy6 = accuracy_score(y_train6, predictions_train6)\n","train_accuracy7 = accuracy_score(y_train7, predictions_train7)\n","train_accuracy8 = accuracy_score(y_train8, predictions_train8)\n","train_accuracy9 = accuracy_score(y_train9, predictions_train9)\n","train_accuracy10 = accuracy_score(y_train10, predictions_train10)\n","train_accuracy = (train_accuracy1 + train_accuracy2 + train_accuracy3 + train_accuracy4 + train_accuracy5 + train_accuracy6 + train_accuracy7 + train_accuracy8 + train_accuracy9 + train_accuracy10)/10\n","print(f\"Avg Train Accuracy: {train_accuracy}\")\n","\n","# Predicting on the valid set\n","predictions_valid1 = rf_classifier1.predict(X_valid1)\n","predictions_valid2 = rf_classifier2.predict(X_valid2)\n","predictions_valid3 = rf_classifier3.predict(X_valid3)\n","predictions_valid4 = rf_classifier4.predict(X_valid4)\n","predictions_valid5 = rf_classifier5.predict(X_valid5)\n","predictions_valid6 = rf_classifier5.predict(X_valid6)\n","predictions_valid7 = rf_classifier5.predict(X_valid7)\n","predictions_valid8 = rf_classifier5.predict(X_valid8)\n","predictions_valid9 = rf_classifier5.predict(X_valid9)\n","predictions_valid10 = rf_classifier5.predict(X_valid10)\n","\n","valid_accuracy1 = accuracy_score(y_valid1, predictions_valid1)\n","valid_accuracy2 = accuracy_score(y_valid2, predictions_valid2)\n","valid_accuracy3 = accuracy_score(y_valid3, predictions_valid3)\n","valid_accuracy4 = accuracy_score(y_valid4, predictions_valid4)\n","valid_accuracy5 = accuracy_score(y_valid5, predictions_valid5)\n","valid_accuracy6 = accuracy_score(y_valid6, predictions_valid6)\n","valid_accuracy7 = accuracy_score(y_valid7, predictions_valid7)\n","valid_accuracy8 = accuracy_score(y_valid8, predictions_valid8)\n","valid_accuracy9 = accuracy_score(y_valid9, predictions_valid9)\n","valid_accuracy10 = accuracy_score(y_valid10, predictions_valid10)\n","valid_accuracy = (valid_accuracy1 + valid_accuracy2 + valid_accuracy3 + valid_accuracy4 + valid_accuracy5 + valid_accuracy6 + valid_accuracy7 + valid_accuracy8 + valid_accuracy9 + valid_accuracy10)/10\n","print(f\"Avg Valid Accuracy: {valid_accuracy}\")\n","\n","# Predicting on the test set\n","predictions_test1 = rf_classifier1.predict(X_test)\n","predictions_test2 = rf_classifier2.predict(X_test)\n","predictions_test3 = rf_classifier3.predict(X_test)\n","predictions_test4 = rf_classifier4.predict(X_test)\n","predictions_test5 = rf_classifier5.predict(X_test)\n","predictions_test6 = rf_classifier6.predict(X_test)\n","predictions_test7 = rf_classifier7.predict(X_test)\n","predictions_test8 = rf_classifier8.predict(X_test)\n","predictions_test9 = rf_classifier9.predict(X_test)\n","predictions_test10 = rf_classifier10.predict(X_test)\n","\n","test_accuracy1 = accuracy_score(y_test, predictions_test1)\n","test_accuracy2 = accuracy_score(y_test, predictions_test2)\n","test_accuracy3 = accuracy_score(y_test, predictions_test3)\n","test_accuracy4 = accuracy_score(y_test, predictions_test4)\n","test_accuracy5 = accuracy_score(y_test, predictions_test5)\n","test_accuracy6 = accuracy_score(y_test, predictions_test6)\n","test_accuracy7 = accuracy_score(y_test, predictions_test7)\n","test_accuracy8 = accuracy_score(y_test, predictions_test8)\n","test_accuracy9 = accuracy_score(y_test, predictions_test9)\n","test_accuracy10 = accuracy_score(y_test, predictions_test10)\n","test_accuracy = (test_accuracy1 + test_accuracy2 + test_accuracy3 + test_accuracy4 + test_accuracy5 + test_accuracy6 + test_accuracy7 + test_accuracy8 + test_accuracy9 + test_accuracy10)/10\n","print(f\"Avg Test Accuracy: {test_accuracy}\")\n","\n","pooled_predictions = []\n","for i in range(len(predictions_test1)):\n","  predicted_true = 0\n","  predicted_false = 0\n","  for prediction_list in [predictions_test1, predictions_test2, predictions_test3, predictions_test4, predictions_test5, predictions_test6, predictions_test7, predictions_test8, predictions_test9, predictions_test10]:\n","    if prediction_list[i] == 'T':\n","       predicted_true += 1\n","    else:\n","      predicted_false += 1\n","  if predicted_true > predicted_false:\n","    pooled_predictions.append('T')\n","  else:\n","    pooled_predictions.append('F')\n","\n","pooled_test_accuracy = accuracy_score(y_test, pooled_predictions)\n","print(f\"Test Accuracy With Pooling: {pooled_test_accuracy}\")\n","\n","print(\"Criterion:\", best_criterion)\n","print(\"Min Samples To Split:\", best_min_samples_split)\n","print(\"Max Features:\", best_max_features)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_xpI_8wV7QEB","outputId":"cd654023-a0a0-4357-c437-b6c903a83eec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Progress = 0.5 %\n","Progress = 1.75 %\n","Progress = 3.0 %\n","Progress = 4.25 %\n","Progress = 5.5 %\n","Progress = 6.75 %\n","Progress = 8.0 %\n","Progress = 9.25 %\n","Progress = 10.5 %\n","Progress = 11.75 %\n","Progress = 13.0 %\n","Progress = 14.25 %\n","Progress = 15.5 %\n","Progress = 16.75 %\n","Progress = 18.0 %\n","Progress = 19.25 %\n","Progress = 20.5 %\n","Progress = 21.75 %\n","Progress = 23.0 %\n","Progress = 24.25 %\n","Progress = 25.5 %\n","Progress = 26.75 %\n","Progress = 28.0 %\n","Progress = 29.25 %\n","Progress = 30.5 %\n","Progress = 31.75 %\n","Progress = 33.0 %\n","Progress = 34.25 %\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMmIiYZfJn/AmFHHfzNIiMZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}